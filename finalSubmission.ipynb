{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code for Final Prediction Competition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np, pandas as pd\n",
    "from sklearn.model_selection import (train_test_split , GridSearchCV)\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PreProcessing Section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in Data\n",
    "data = pd.read_csv(\"./Econ424_F2023_PC6_glassdoor_training_small_v1.csv\")\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop(['small'], axis=\"columns\",inplace=True)\n",
    "data.drop(columns=[\"location\",\"firm\",\"date_review\"],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output to csv file\n",
    "csv_file_out = \"./preprocessing.csv\"\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "data.to_csv(csv_file_out,index=False, encoding=\"utf-8\", float_format=\"%1.6f\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"./preprocessing.csv\", lineterminator='\\n')\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the columns you want to check for missing values\n",
    "columns_to_check = ['pros', 'cons', 'headline']\n",
    "\n",
    "# Check for missing values in the specified columns\n",
    "df = df.dropna(subset=columns_to_check)\n",
    "missing_values = df[columns_to_check].isna()\n",
    "rows_with_missing_values = df[missing_values.any(axis=1)]\n",
    "print(len(rows_with_missing_values))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis Feature Construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.sentiment import SentimentIntensityAnalyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['pros'] = df['pros'].astype(str)\n",
    "df['cons'] = df['cons'].astype(str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature construction\n",
    "df['pros_length'] = df['pros'].apply(len)\n",
    "df['cons_length'] = df['cons'].apply(len)\n",
    "df['headline_sentiment'] = df['headline'].apply(lambda x: SentimentIntensityAnalyzer().polarity_scores(str(x))['compound'])\n",
    "df['pros_sentiment'] = df['pros'].apply(lambda x: SentimentIntensityAnalyzer().polarity_scores(str(x))['compound'])\n",
    "df['cons_sentiment'] = df['cons'].apply(lambda x: SentimentIntensityAnalyzer().polarity_scores(str(x))['compound'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check missing values\n",
    "print(df[\"headline_sentiment\"].isna().values.any())\n",
    "print(df[\"pros_sentiment\"].isna().values.any())\n",
    "print(df[\"cons_sentiment\"].isna().values.any())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Progress\n",
    "csv_file_out = \"./postsentiment.csv\"\n",
    "df.to_csv(csv_file_out,index=False, encoding=\"utf-8\", float_format=\"%1.6f\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"./postsentiment.csv\", lineterminator='\\n')\n",
    "print(df.head())\n",
    "df.drop(columns=[\"headline\"],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Features and target variable\n",
    "features = ['pros_length', 'cons_length', 'headline_sentiment', 'pros_sentiment', 'cons_sentiment','year']\n",
    "target = 'overall_rating'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify our x and y\n",
    "y = df['overall_rating']\n",
    "X = df.drop(columns=['overall_rating','pros','cons','job_title'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Neural Net MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Building with Sigmoid Neuron\n",
    "model = MLPRegressor(hidden_layer_sizes=(100), activation='logistic', max_iter=500, solver='adam')\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "# y_train_pred = np.clip(model.predict(X_train),1, 5)\n",
    "# y_test_pred = np.clip(model.predict(X_test),1,5)\n",
    "y_train_pred = np.round(np.clip(model.predict(X_train),1, 5))\n",
    "y_test_pred = np.round(np.clip(model.predict(X_test),1,5))\n",
    "\n",
    "# Model Evaluation\n",
    "mse_train = mean_squared_error(y_train, y_train_pred)\n",
    "mse_test = mean_squared_error(y_test, y_test_pred)\n",
    "r2_train = r2_score(y_train, y_train_pred)\n",
    "r2_test = r2_score(y_test, y_test_pred)\n",
    "\n",
    "# Print MSE and R2 for the training set\n",
    "print(f'MSE (Training Set): {mse_train}')\n",
    "print(f'R2 Score (Training Set): {r2_train}')\n",
    "\n",
    "\n",
    "# Print MSE and R2 for the training set\n",
    "print(f'MSE (Test Set): {mse_test}')\n",
    "print(f'R2 Score (Test Set): {r2_test}')\n",
    "\n",
    "y_train_pred.dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Random Forest Classifier\n",
    "rf_classifier = RandomForestClassifier()\n",
    "\n",
    "# Define the hyperparameters and their possible values\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 150],  # Adjust these values based on your needs\n",
    "    'max_depth': [None, 1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17],  # Other hyperparameters you want to tune\n",
    "    # Add more hyperparameters as needed\n",
    "}\n",
    "\n",
    "# Create the GridSearchCV object\n",
    "grid_search = GridSearchCV(estimator=rf_classifier, param_grid=param_grid, cv=5, scoring='accuracy')\n",
    "\n",
    "# Fit the model to the training data\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Print the best hyperparameters\n",
    "print(\"Best Hyperparameters:\", grid_search.best_params_)\n",
    "\n",
    "# Get the best model\n",
    "best_rf_model = grid_search.best_estimator_\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "test_accuracy = best_rf_model.score(X_test, y_test)\n",
    "print(\"Test Accuracy:\", test_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract Predictions\n",
    "y_train_pred_rf = best_rf_model.predict(X_train)\n",
    "y_test_pred_rf = best_rf_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse_train_rf = mean_squared_error(y_train, y_train_pred_rf)\n",
    "mse_test_rf = mean_squared_error(y_test, y_test_pred_rf)\n",
    "r2_train_rf = r2_score(y_train, y_train_pred_rf)\n",
    "r2_test_rf = r2_score(y_test, y_test_pred_rf)\n",
    "\n",
    "# Print MSE and R2 for the training set\n",
    "print(f'MSE (Training Set): {mse_train_rf}')\n",
    "print(f'R2 Score (Training Set): {r2_train_rf}')\n",
    "\n",
    "\n",
    "# Print MSE and R2 for the training set\n",
    "print(f'MSE (Test Set): {mse_test_rf}')\n",
    "print(f'R2 Score (Test Set): {r2_test_rf}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graph Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Importance for Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initalize the feature importance from the best tree\n",
    "feature_imp = pd.DataFrame( {'importance':best_rf_model.feature_importances_}, index=features)\n",
    "feature_imp.sort_values(by='importance', ascending=True)\n",
    "\n",
    "# Sort the names and importances\n",
    "sorted_names, sorted_imp = zip(*sorted(zip(features, feature_imp['importance']), key=lambda x: x[1]))\n",
    "\n",
    "# Plot the bar graph\n",
    "plt.barh(sorted_names, sorted_imp, label='Importance', color='red')\n",
    "plt.xlabel(\"Importance\")\n",
    "plt.ylabel(\"Variable\")\n",
    "plt.title(\"Variable Importance Plot\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_matrix_train_rf = confusion_matrix(y_train, y_train_pred_rf)\n",
    "\n",
    "# Plot Confusion Matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(conf_matrix_train_rf, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=[1, 2, 3, 4, 5], yticklabels=[1, 2, 3, 4, 5])\n",
    "plt.title('Confusion Matrix - Training Data')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction vs Actual Distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Consolidated prediction distribution graph\n",
    "fig, axes = plt.subplots(1, 2, figsize=(20, 10))\n",
    "fig.suptitle('Predicted vs Actual Distributions', fontsize=16)\n",
    "# Plot prediction distributions for actual and predicted values in training and test sets\n",
    "sns.histplot(y_test, label='Actual (Test)', ax=axes[0], kde=False, color = \"red\")\n",
    "sns.histplot(y_test_pred_rf, label='Predicted (Test)', ax=axes[0], kde=False, color=\"yellow\")\n",
    "axes[0].set_title(f'Prediction Distribution for Test Set')\n",
    "axes[0].legend()\n",
    "\n",
    "sns.histplot(y_train, label='Actual (Train)', ax=axes[1], kde=False, color = \"green\")\n",
    "sns.histplot(y_train_pred_rf, label='Predicted (Train)', ax=axes[1], kde=False, color=\"skyblue\")\n",
    "axes[1].set_title(f'Prediction Distribution for Training Set')\n",
    "axes[1].legend()\n",
    "# sns.histplot(Y_test_pred, label='Predicted (Test)', ax=axes, kde=False, color=\"red\")\n",
    "\n",
    "\n",
    "# Save the figure\n",
    "plt.savefig('consolidated_prediction_distributions.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Consolidated feature distribution graph\n",
    "fig, axes = plt.subplots(3, 2, figsize=(15, 15))\n",
    "fig.suptitle('Feature Distributions', fontsize=16)\n",
    "feature_cols = ['pros_length', 'cons_length', 'headline_sentiment', 'pros_sentiment', 'cons_sentiment','year']\n",
    "\n",
    "# Plot feature distributions for training and test sets\n",
    "for i, feature in enumerate(feature_cols):\n",
    "    \n",
    "    x = math.floor(i/2)\n",
    "    y = i%(2)\n",
    "    sns.histplot(X_train[feature], ax=axes[x, y],label='Train', kde=False)\n",
    "    sns.histplot(X_test[feature], ax=axes[x, y],label='Test', kde=False)\n",
    "    axes[x, y].set_title(f'{feature} Distribution')\n",
    "    axes[x, y].legend()\n",
    "axes[0, 0].set_xlim(0, 1050)\n",
    "axes[0, 1].set_xlim(0, 1400) \n",
    "axes[1, 0].set_xlim(-1, 1)\n",
    "axes[1, 1].set_xlim(-1, 1) \n",
    "axes[2,0].set_xlim(-1, 1) \n",
    "# Remove the empty subplot in the last row and second column\n",
    "# fig.delaxes(axes[2, 1])\n",
    "\n",
    "# Adjust layout to prevent clipping of titles\n",
    "fig.tight_layout()\n",
    "\n",
    "# Save the figure\n",
    "plt.savefig('consolidated_feature_distributions.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlation Heat Map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation heat-map\n",
    "correlation_matrix = df[['pros_length', 'cons_length', 'headline_sentiment', 'pros_sentiment', 'cons_sentiment','year', 'overall_rating']].corr()\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=\".2f\", vmin=-1, vmax=1)\n",
    "plt.title('Correlation Heat-map')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom Map: Violin Chart for Sentiment Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Consolidated feature distribution graph\n",
    "fig, axes = plt.subplots(nrows=3, figsize=(10, 21))\n",
    "fig.suptitle('Sentiment Score vs Overall Rating', fontsize=16)\n",
    "feature_cols = ['headline_sentiment', 'pros_sentiment', 'cons_sentiment']\n",
    "\n",
    "# Plot feature distributions for training and test sets\n",
    "for i, feature in enumerate(feature_cols):\n",
    "    # Create a violin plot\n",
    "    sns.violinplot(x='overall_rating', y=feature, data=df, ax=axes[i], palette='coolwarm', scale='width')\n",
    "    axes[i].set_title(f'Distribution of {feature} Score for Overall Rating')\n",
    "\n",
    "# Adjust the vertical gap between subplots\n",
    "plt.subplots_adjust(hspace=0.2)\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predictions on No Response Dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataPred = pd.read_csv(\"./424_F2023_Final_PC_glassdoor_test_without_response_v1.csv\")\n",
    "print(dataPred.head())\n",
    "print(dataPred.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataPred.drop(['overall_rating','small','job_title','location'], errors='ignore',\n",
    "  axis='columns', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the columns you want to check for missing values\n",
    "columns_to_check = ['pros', 'cons', 'headline','year']\n",
    "\n",
    "# Check for missing values in the specified columns\n",
    "dataPred[columns_to_check].isna().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill empty values since there's only a few\n",
    "dataPred['cons'].fillna('', inplace=True)\n",
    "dataPred['headline'].fillna('', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dataPred['pros_length'] = dataPred['pros'].apply(len)\n",
    "dataPred['cons_length'] = dataPred['cons'].apply(len)\n",
    "dataPred['headline_sentiment'] = dataPred['headline'].apply(lambda x: SentimentIntensityAnalyzer().polarity_scores(str(x))['compound'])\n",
    "dataPred['pros_sentiment'] = dataPred['pros'].apply(lambda x: SentimentIntensityAnalyzer().polarity_scores(str(x))['compound'])\n",
    "dataPred['cons_sentiment'] = dataPred['cons'].apply(lambda x: SentimentIntensityAnalyzer().polarity_scores(str(x))['compound'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save progress\n",
    "csv_file_out = \"./postsentimentTestNew.csv\"\n",
    "dataPred.to_csv(csv_file_out,index=False, encoding=\"utf-8\", float_format=\"%1.6f\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataPred = dataPred[['year', 'pros_length', 'cons_length', 'headline_sentiment',\n",
    "       'pros_sentiment', 'cons_sentiment']]\n",
    "Y_test_pred = best_rf_model.predict(dataPred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Predictions\n",
    "csv_file_out = \"./outputRF.csv\"\n",
    "np.savetxt(csv_file_out, Y_test_pred, delimiter=\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Actionable Insights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.probability import FreqDist\n",
    "from nltk.stem import PorterStemmer\n",
    "import string\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.util import ngrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load your dataset (replace 'your_dataset.csv' with your actual file)\n",
    "df = pd.read_csv('postsentiment.csv',lineterminator='\\n')\n",
    "\n",
    "# Separate data into high and low ratings\n",
    "high_ratings = df[(df['overall_rating'] == 4) | (df['overall_rating'] == 5)]\n",
    "low_ratings = df[(df['overall_rating'] == 1) | (df['overall_rating'] == 2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Function for text processing\n",
    "def process_text(text):\n",
    "    # Tokenize\n",
    "    tokens = word_tokenize(text)\n",
    "    # Convert to lowercase\n",
    "    tokens = [word.lower() for word in tokens]\n",
    "    # Remove punctuation\n",
    "    tokens = [word for word in tokens if word.isalpha()]\n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "    # Stemming (you can also use lemmatization)\n",
    "    porter = PorterStemmer()\n",
    "    tokens = [porter.stem(word) for word in tokens]\n",
    "    return tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process pros text for high and low ratings\n",
    "high_pros_tokens = high_ratings['pros'].apply(process_text)\n",
    "low_pros_tokens = low_ratings['pros'].apply(process_text)\n",
    "\n",
    "# Process cons text for high and low ratings\n",
    "high_cons_tokens = high_ratings['cons'].apply(process_text)\n",
    "low_cons_tokens = low_ratings['cons'].apply(process_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate and Extract Common Phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate n-grams\n",
    "def extract_ngrams(tokens, n):\n",
    "    return list(ngrams(tokens, n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract n-grams for high and low ratings\n",
    "high_pros_ngrams = [phrase for tokens in high_pros_tokens for phrase in extract_ngrams(tokens, 3)]\n",
    "low_pros_ngrams = [phrase for tokens in low_pros_tokens for phrase in extract_ngrams(tokens, 3)]\n",
    "\n",
    "# Convert n-grams to strings for easier analysis\n",
    "high_pros_ngram_strings = [' '.join(phrase) for phrase in high_pros_ngrams]\n",
    "low_pros_ngram_strings = [' '.join(phrase) for phrase in low_pros_ngrams]\n",
    "\n",
    "# Calculate frequency of n-grams\n",
    "high_pros_ngram_freq = pd.Series(high_pros_ngram_strings).value_counts()\n",
    "low_pros_ngram_freq = pd.Series(low_pros_ngram_strings).value_counts()\n",
    "\n",
    "# Extract common n-grams\n",
    "common_high_pros_ngrams = high_pros_ngram_freq.head(10).index.tolist()\n",
    "common_low_pros_ngrams = low_pros_ngram_freq.head(10).index.tolist()\n",
    "\n",
    "print(\"Common phrases in high-rated pros:\", common_high_pros_ngrams)\n",
    "print(\"Common phrases in low-rated pros:\", common_low_pros_ngrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Repeat for cons\n",
    "\n",
    "# Extract n-grams for high and low ratings\n",
    "high_cons_ngrams = [phrase for tokens in high_cons_tokens for phrase in extract_ngrams(tokens, 2)]\n",
    "low_cons_ngrams = [phrase for tokens in low_cons_tokens for phrase in extract_ngrams(tokens, 2)]\n",
    "\n",
    "# Convert n-grams to strings for easier analysis\n",
    "high_cons_ngram_strings = [' '.join(phrase) for phrase in high_cons_ngrams]\n",
    "low_cons_ngram_strings = [' '.join(phrase) for phrase in low_cons_ngrams]\n",
    "\n",
    "# Calculate frequency of n-grams\n",
    "high_cons_ngram_freq = pd.Series(high_cons_ngram_strings).value_counts()\n",
    "low_cons_ngram_freq = pd.Series(low_cons_ngram_strings).value_counts()\n",
    "\n",
    "# Extract common n-grams\n",
    "common_high_cons_ngrams = high_cons_ngram_freq.head(10).index.tolist()\n",
    "common_low_cons_ngrams = low_cons_ngram_freq.head(10).index.tolist()\n",
    "\n",
    "print(\"Common phrases in high-rated pros:\", common_high_cons_ngrams)\n",
    "print(\"Common phrases in low-rated pros:\", common_low_cons_ngrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove duplicates for pros\n",
    "indexes = [\"great place work\",\"good work life\",\"great work environ\",\"great work life\", \"life balanc good\", \"good compani work\", \"work environ good\", \"good place work\", \"great work cultur\", \"great peopl great\"]\n",
    "high_pros_bigram_freq = high_pros_bigram_freq.drop(indexes)\n",
    "common_high_pros_bigrams = high_pros_bigram_freq.head(10).index.tolist()\n",
    "highpros = dict(high_pros_bigram_freq[common_high_pros_bigrams])\n",
    "\n",
    "\n",
    "# Remove duplicates for cons\n",
    "\n",
    "indexes = [\"work long hour\", \"work hour week\", \"hour per week\", \"work hour day\", \"work life balanc\"]\n",
    "low_cons_bigram_freq = low_cons_bigram_freq.drop(indexes)\n",
    "common_low_cons_bigrams = low_cons_bigram_freq.head(10).index.tolist()\n",
    "print(\"Common phrases in high-rated pros:\", common_low_cons_bigrams)\n",
    "lowcons = dict(low_cons_bigram_freq[common_low_cons_bigrams])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot Wordclouds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot word clouds\n",
    "def plot_word_cloud(data, title):\n",
    "    wordcloud = WordCloud(width = 500, height = 500, \n",
    "                background_color ='white', \n",
    "                stopwords = set(stopwords.words('english')), \n",
    "                min_font_size = 12).generate_from_frequencies(data)\n",
    "\n",
    "    # plot the WordCloud image                        \n",
    "    plt.figure(figsize = (8, 8), facecolor = None) \n",
    "    plt.imshow(wordcloud) \n",
    "    plt.axis(\"off\") \n",
    "    plt.tight_layout(pad = 0) \n",
    "    plt.title(title)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pros\n",
    "plot_word_cloud((highpros), \"Word Cloud for High-Rated Pros\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cons\n",
    "plot_word_cloud(lowcons, \"Word Cloud for Low-Rated Cons\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "econ424",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
